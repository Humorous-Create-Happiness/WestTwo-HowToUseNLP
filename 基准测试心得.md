## PythonAI第五轮-------使用模型基准测试心得

本轮我们的任务是使用已有的预训练模型，进行学习如何进行语言模型的基准测试，具体到本次任务是给 bert 和各位上一轮的自己写的模型基准测试。

相关信息

中文语言理解测评基准我们采用 CLUE benchmark ，它分为多个测试数据集，最终的测试成绩为所有数据集上成绩的算数平均，本轮各位要运行的数据集有： AFQMC '蚂蚁语义相似度、 IFLYTEK '长文本分类、 TNEWS '今日头条中文新闻（短文本）分类三个数据集，其他数如果有余力可以自行测试并一起提交，其他数据集的下载以及微调时的参数可以参照 CLUE benchmark 的 GitHub 地址。
## 写在前面：因为容量有限，所以不能将模型附上，可以打开test.py直接调用train.py进行使用（可以适当更改训练次数）selftrain.py为独立运行或者可以稍作修改用以test.py

## 1.准备

### 1.1依赖库

```py
from torch import LongTensor 
 from transformers import BertTokenizer , BertModel 
 tokenizer = BertTokenizer . from _ pretrained (" bert - base - chinese ")
 bert = BertModel . from _ pretrained (" bert - base - chinese ")
```

torch库之前已经下载好，在下载transfromers库时要注意不要使用pip install transformers安装了transformers使用以上命令的pytorch环境均会出现[找不到指定的程序]的错误。
请使用pip install transformers -i https://pypi.doubanio.com/simple进行安装，问题解决，不再报错。

### 1.2数据预处理

下载的json文件不能直接使用，要给每一行末尾加一个逗号（最后一行的逗号删除），并在前后加[]

```py
input_file = "test.json"  # 输入文件路径
output_file = "test_processed.json"  # 输出文件路径

# 读取输入文件并转换格式
with open(input_file, "r", encoding="utf-8") as f:
    lines = f.readlines()

# 在每一行的末尾添加逗号
lines = [line.strip() + "," + "\n" for line in lines]

# 保存为新的文件
with open(output_file, "w", encoding="utf-8") as f:
    f.writelines(lines)

print("文件转换完成！")
```

## 2.训练模型！

	

### 2.1 afqmc数据集

AFQMC 蚂蚁金融语义相似度 Ant Financial Question Matching Corpus
   数据量：训练集（34334）验证集（4316）测试集（3861）
   例子：

```py
 {"sentence1": "双十一花呗提额在哪", "sentence2": "里可以提花呗额度", "label": "0"}
```


​     每一条数据有三个属性，从前往后分别是 句子1，句子2，句子相似度标签。其中label标签，1 表示sentence1和sentence2的含义类似，0表示两个句子的含义不同。

#### 2.1.1模型训练与验证

```py
import json
import torch
from torch import LongTensor
from torch import nn
from torch import optim
from transformers import BertTokenizer, BertModel

# 加载BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")
output_dim = 2  # 输出维度（分类数目）                    加载中文bert模型
print('finish loading bert111!')

# 加载数据集(这里不是标准的json格式，需要转换)
def load_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data


# 数据预处理
def preprocess_data(data):
    processed_data = []
    for item in data:
        sentence1 = item["sentence1"]
        sentence2 = item["sentence2"]
        label = int(item["label"])

        # 使用tokenizer对句子进行编码
        encoded_data = tokenizer.encode_plus(
            sentence1,
            sentence2,
            padding="max_length",
            max_length=64,               #根据句子长度可以适当改为128
            truncation=True,
            return_tensors="pt"
        )

        input_ids = encoded_data["input_ids"].squeeze(0)
        attention_mask = encoded_data["attention_mask"].squeeze(0) #遮蔽操作

        processed_data.append({
            "input_ids": input_ids,
            "attention_mask": attention_mask,     
            "label": label
        })                               #对数据处理，标记并进行遮蔽
    print('finish,loading data!')
    return processed_data


# 创建分类模型
class Classifier(nn.Module):
    def __init__(self, bert, output_dim):
        super(Classifier, self).__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.2)      #0.2概率失活
        self.fc = nn.Linear(bert.config.hidden_size, output_dim)#转化为线性层并前向传播

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask).last_hidden_state
        output = self.dropout(outputs[:, 0, :])
        output = self.fc(output)
        return output


# 训练模型
def train_model(model, train_data, dev_data):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    #device = torch.device("cpu")
    model.to(device)

    criterion = nn.CrossEntropyLoss()                  #训练标准
    optimizer = optim.Adam(model.parameters(), lr=1e-5)#构建优化学习器

    best_acc = 0.0

    for epoch in range(10):  # 迭代次数
        model.train()
        train_loss = 0.0
        train_correct = 0
        #装载数据，进行训练
        for item in train_data:
            input_ids = item["input_ids"].unsqueeze(0).to(device)
            attention_mask = item["attention_mask"].unsqueeze(0).to(device)
            label = torch.tensor(item["label"]).unsqueeze(0).to(device)

            optimizer.zero_grad()

            output = model(input_ids, attention_mask)
            _, predicted = torch.max(output, 1)

            loss = criterion(output, label)
            loss.backward()              #损失前向传播
            optimizer.step()             #使用优化器更新参数

            train_loss += loss.item() * input_ids.size(0)
            train_correct += (predicted == label).sum().item()

            step = 0
            if step % 1000 == 1:
                print(f"Step: {step}/{len(train_data)}")
            step += 1

        train_loss /= len(train_data)
        train_acc = train_correct / len(train_data)

        # 在验证集上进行评估
        val_loss, val_acc = evaluate_model(model, dev_data)

        print(f"Epoch: {epoch + 1}/{10}")
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print()

        # 保存在验证集上表现最好的模型
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), "best_model1.pt")

    print("Training finished.")
    print("Best validation accuracy: ", best_acc)


# 在验证集上评估模型
def evaluate_model(model, data):
    #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device = torch.device("cpu")
    model.eval()

    total_loss = 0.0
    total_correct = 0

    with torch.no_grad():
        for item in data:
            input_ids = item["input_ids"].unsqueeze(0).to(device)
            attention_mask = item["attention_mask"].unsqueeze(0).to(device)
            label = torch.tensor(item["label"]).unsqueeze(0).to(device)

            output = model(input_ids, attention_mask)
            _, predicted = torch.max(output, 1)

            loss = nn.CrossEntropyLoss()(output, label)    #计算损失

            total_loss += loss.item() * input_ids.size(0)
            total_correct += (predicted == label).sum().item()

            step = 0
            if step % 1000 == 1:
                print(f"EStep: {step}/{len(data)}")
            step += 1

    avg_loss = total_loss / len(data)
    accuracy = total_correct / len(data)

    return avg_loss, accuracy



# 设置文件路径
train_file = "train_processed.json"  # 训练集文件路径
dev_file = "dev_processed.json"  # 验证集文件路径
test_file = "test_processed.json"  # 测试集文件路径

# 加载并预处理数据
train_data = load_data(train_file)
train_data = preprocess_data(train_data)

dev_data = load_data(dev_file)
dev_data = preprocess_data(dev_data)

# 创建分类模型实例
model = Classifier(bert, output_dim)
print('finish classfied')
# 训练模型
train_model(model, train_data, dev_data)
print('finish model')
# 加载在验证集上表现最好的模型
model.load_state_dict(torch.load("best_model1.pt"))

# 评估测试集
#test_accuracy, test_results = evaluate_test_data(model, test_data)

print('finish model load to best_model1.pt')

```

在10个epoch之后训练集正确率为63.2%,验证集正确率为64.8%



#### 2.1.2模型测试输出

ps.事实上只要运行test文件就行了（会自动加载train文件）

```py
import json
import torch
from transformers import BertTokenizer, BertModel
from train import Classifier                    #装载先前训练的文件

# 加载BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")
output_dim = 2  # 输出维度（分类数目）
print('finish loading bert!')

# 加载保存的模型
model = Classifier(bert, output_dim)
model.load_state_dict(torch.load("best_model1.pt"))
model.eval()
print('finish loading model!')

# 加载测试数据
with open("test_processed.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)
print('finish loading json!')

# 预测并保存结果
for item in test_data:
    sentence1 = item["sentence1"]
    sentence2 = item["sentence2"]

    # 使用tokenizer对句子进行编码
    encoded_data = tokenizer.encode_plus(
        sentence1,
        sentence2,
        padding="max_length",
        max_length=64,
        truncation=True,
        return_tensors="pt"
    )

    input_ids = encoded_data["input_ids"].squeeze(0)
    attention_mask = encoded_data["attention_mask"].squeeze(0)

    step = 0
    if step % 1000 == 1:
        print(f"Step: {step}/{len(test_data)}")
    step += 1

    # 预测标签
    with torch.no_grad():
        output = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))
        _, predicted = torch.max(output, 1)
        predicted_label = predicted.item()

    # 将预测结果保存到label字段并输出到test.josn文件中
    item["label"] = predicted_label

# 保存结果到test.json文件
with open("test.json", "w", encoding="utf-8") as f:
    json.dump(test_data, f, ensure_ascii=False, indent=4)

print("预测结果已保存到test_process.json文件。")

```

在结果中我注意到模型可能过拟合了，故降低学习率至1e-6进行学习





### 2.2 iflytek数据集

FLYTEK' 长文本分类 Long Text classification
该数据集共有1.7万多条关于app应用描述的长文本标注数据，包含和日常生活相关的各类应用主题，共119个类别："打车":0,"地图导航":1,"免费WIFI":2,"租车":3,….,"女性":115,"经营":116,"收款":117,"其他":118(分别用0-118表示)。

  数据量：训练集(12,133)，验证集(2,599)，测试集(2,600)

```py 
{"label": "110", "label_des": "社区超市", "sentence": "朴朴快送超市创立于2016年，专注于打造移动端30分钟即时配送一站式购物平台，商品品类包含水果、蔬菜、肉禽蛋奶、海鲜水产、粮油调味、酒水饮料、休闲食品、日用品、外卖等。朴朴公司希望能以全新的商业模式，更高效快捷的仓储配送模式，致力于成为更快、更好、更多、更省的在线零售平台，带给消费者更好的消费体验，同时推动中国食品安全进程，成为一家让社会尊敬的互联网公司。,朴朴一下，又好又快,1.配送时间提示更加清晰友好2.保障用户隐私的一些优化3.其他提高使用体验的调整4.修复了一些已知bug"}
```


  每一条数据有三个属性，从前往后分别是 类别ID，类别名称，文本内容。

#### 2.2.1模型训练与验证

与2.1.1相似，但注意到实际有119个标签，

```py
import json
import torch
from torch import LongTensor
from torch import nn
from torch import optim
from transformers import BertTokenizer, BertModel

# 加载BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")
output_dim = 119  #------------------------------------# 输出维度（分类数目）修改


# 加载数据集
def load_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data


# 数据预处理
def preprocess_data(data):
    processed_data = []
    for item in data:
        label = int(item["label"])  #0-118
        sentence = item["sentence"]

        # 使用tokenizer对句子进行编码
        encoded_data = tokenizer.encode_plus(
            sentence,                       #--------------只需要加载一个sentence
            padding="max_length",  
            max_length=64,
            truncation=True,
            return_tensors="pt"
        )

        input_ids = encoded_data["input_ids"].squeeze(0)
        attention_mask = encoded_data["attention_mask"].squeeze(0)

        processed_data.append({
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "label": label
        })
    return processed_data


# 创建分类模型
class Classifier(nn.Module):
    def __init__(self, bert, output_dim):
        super(Classifier, self).__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(bert.config.hidden_size, output_dim)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        output = self.dropout(pooled_output)
        output = self.fc(output)                #---------- 修改forward以输出正确张量
        return output


# 训练模型
def train_model(model, train_data, dev_data):
    #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device = torch.device("cpu")
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-5)

    best_acc = 0.0

    for epoch in range(3):  # 迭代次数
        print(f"Epoch: {epoch + 1}/{10}")
        model.train()
        train_loss = 0.0
        train_correct = 0
        step = 0
        for item in train_data:
            input_ids = item["input_ids"].unsqueeze(0).to(device)
            attention_mask = item["attention_mask"].unsqueeze(0).to(device)
            label = torch.tensor(item["label"]).unsqueeze(0).to(device)

            optimizer.zero_grad()

            output = model(input_ids, attention_mask)
            _, predicted = torch.max(output, 1)

            loss = criterion(output, label)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * input_ids.size(0)
            train_correct += (predicted == label).sum().item()
            if step%1000 ==1:
                print(f"Step: {step}/{len(train_data)}")
            step+=1
        train_loss /= len(train_data)
        train_acc = train_correct / len(train_data)

        # 在验证集上进行评估
        val_loss, val_acc = evaluate_model(model, dev_data)


        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print()

        # 保存在验证集上表现最好的模型
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), "best_model2.pt")

    print("Training finished.")
    print("Best validation accuracy: ", best_acc)


# 在验证集上评估模型
def evaluate_model(model, data):
    #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device = torch.device("cpu")
    model.eval()

    total_loss = 0.0
    total_correct = 0

    with torch.no_grad():
        for item in data:
            input_ids = item["input_ids"].unsqueeze(0).to(device)
            attention_mask = item["attention_mask"].unsqueeze(0).to(device)
            label = torch.tensor(item["label"]).unsqueeze(0).to(device)

            output = model(input_ids, attention_mask)
            _, predicted = torch.max(output, 1)

            loss = nn.CrossEntropyLoss()(output, label)

            total_loss += loss.item() * input_ids.size(0)
            total_correct += (predicted == label).sum().item()

    avg_loss = total_loss / len(data)
    accuracy = total_correct / len(data)

    return avg_loss, accuracy





# 设置文件路径
train_file = "train_processed.json"  # 训练集文件路径
dev_file = "dev_processed.json"  # 验证集文件路径


# 加载并预处理数据
train_data = load_data(train_file)
train_data = preprocess_data(train_data)

dev_data = load_data(dev_file)
dev_data = preprocess_data(dev_data)

#test_data = load_data(test_file)
#test_data = preprocess_data(test_data)

# 创建分类模型实例
model = Classifier(bert, output_dim)

# 训练模型
train_model(model, train_data, dev_data)

# 加载在验证集上表现最好的模型
model.load_state_dict(torch.load("best_model3.pt"))

# 评估测试集
#test_accuracy, test_results = evaluate_test_data(model, test_data)

# 打印测试集正确率
#print("Test Accuracy:", test_accuracy)

# 打印测试集结果
#print("Test Results:")
#print(test_results[0])

```

在10个epoch之后训练集正确率为60.2%,验证集正确率为61.8%

#### 2.2.2模型测试输出

```py
import json
import torch
from torch import LongTensor
from transformers import BertTokenizer, BertModel
from train import Classifier

# 加载BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")
output_dim = 119  # 输出维度（分类数目）
print('finish loading bert!')

# 加载保存的模型
model = Classifier(bert, output_dim)
model.load_state_dict(torch.load("best_model2.pt"))
model.eval()
print('finish loading model!')

# 加载标签映射文件，---------------------这里直接转为int型
with open("labels.json", "r", encoding="utf-8") as f:
    label_data = json.load(f)
label_mapping = {int(item["label"]): item["label_des"] for item in label_data}

# 加载测试数据
with open("test_processed.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)
print('finish loading json!')
# 预测并转换标签为label_desc
for item in test_data:
    sentence = item["sentence"]
    keywords = item["keywords"]

    # 使用tokenizer对句子进行编码
    encoded_data = tokenizer.encode_plus(
        sentence,
        padding="max_length",
        max_length=64,
        truncation=True,
        return_tensors="pt"
    )

    input_ids = encoded_data["input_ids"].squeeze(0)
    attention_mask = encoded_data["attention_mask"].squeeze(0)
    step=0
    if step % 1000 == 1:
        print(f"Step: {step}/{len(test_data)}")
    step += 1

    # 预测标签
    with torch.no_grad():
        output = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))
        _, predicted = torch.max(output, 1)
        predicted_label = predicted.item()

    # 转换为label_desc
    label_desc = label_mapping[str(predicted_label)]

    # 将label_desc输出到keywords字段
    item["keywords"] = label_desc

# 保存结果到test.json文件
with open("test_processed.json", "w", encoding="utf-8") as f:
    json.dump(test_data, f, ensure_ascii=False, indent=4)

print("预测结果已保存到test.json文件。")

```



### 2.3 tnews数据集

TNEWS' 今日头条中文新闻（短文本）分类 Short Text Classificaiton for News
该数据集来自今日头条的新闻版块，共提取了15个类别的新闻，包括旅游，教育，金融，军事等。

   数据量：训练集(53,360)，验证集(10,000)，测试集(10,000)

```py
 例子：
   {"label": "102", "label_des": "news_entertainment", "sentence": "江疏影甜甜圈自拍，迷之角度竟这么好看，美吸引一切事物"}
```

  



#### 2.3.1模型训练与验证

与2.1.1相似，但注意到实际有17个标签，(坑了一下)，由于性质与iflynet类似，甚至不需要过多修改

```py
import json
import torch
from torch import LongTensor
from torch import nn
from torch import optim
from transformers import BertTokenizer, BertModel

# 加载BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")
output_dim = 17  -#---------------------------------------- 输出维度（分类数目）


# 加载数据集
def load_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data


# 数据预处理
def preprocess_data(data):
    processed_data = []
    for item in data:
        label = int(item["label"])-100  #100-116
        sentence = item["sentence"]

        # 使用tokenizer对句子进行编码
        encoded_data = tokenizer.encode_plus(
            sentence,
            padding="max_length",
            max_length=64,
            truncation=True,
            return_tensors="pt"
        )

        input_ids = encoded_data["input_ids"].squeeze(0)
        attention_mask = encoded_data["attention_mask"].squeeze(0)

        processed_data.append({
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "label": label
        })
    return processed_data


# 创建分类模型
class Classifier(nn.Module):
    def __init__(self, bert, output_dim):
        super(Classifier, self).__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(bert.config.hidden_size, output_dim)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        output = self.dropout(pooled_output)
        output = self.fc(output)
        return output


# 训练模型
def train_model(model, train_data, dev_data):
    #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device = torch.device("cpu")
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-5)

    best_acc = 0.0

    for epoch in range(10):  # 迭代次数
        model.train()
        train_loss = 0.0
        train_correct = 0

        for item in train_data:
            input_ids = item["input_ids"].unsqueeze(0).to(device)
            attention_mask = item["attention_mask"].unsqueeze(0).to(device)
            label = torch.tensor(item["label"]).unsqueeze(0).to(device)

            optimizer.zero_grad()

            output = model(input_ids, attention_mask)
            _, predicted = torch.max(output, 1)

            loss = criterion(output, label)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * input_ids.size(0)
            train_correct += (predicted == label).sum().item()

        train_loss /= len(train_data)
        train_acc = train_correct / len(train_data)

        # 在验证集上进行评估
        val_loss, val_acc = evaluate_model(model, dev_data)

        print(f"Epoch: {epoch + 1}/{10}")
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print()

        # 保存在验证集上表现最好的模型
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), "best_model2.pt")

    print("Training finished.")
    print("Best validation accuracy: ", best_acc)




# 设置文件路径
train_file = "train_processed.json"  # 训练集文件路径
dev_file = "dev_processed.json"  # 验证集文件路径


# 加载并预处理数据
train_data = load_data(train_file)
train_data = preprocess_data(train_data)

dev_data = load_data(dev_file)
dev_data = preprocess_data(dev_data)


# 创建分类模型实例
model = Classifier(bert, output_dim)

# 训练模型
train_model(model, train_data, dev_data)

# 加载在验证集上表现最好的模型
model.load_state_dict(torch.load("best_model2.pt"))


```

在10个epoch之后训练集正确率为55.2%,验证集正确率为57.8%



#### 2.3.2模型测试输出

```py
import json
import torch
from torch import LongTensor
from transformers import BertTokenizer, BertModel
from train import Classifier

# 加载BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")
output_dim = 17  # 输出维度（分类数目）
print('finish loading bert!')

# 加载保存的模型
model = Classifier(bert, output_dim)
model.load_state_dict(torch.load("best_model2.pt"))
model.eval()
print('finish loading model!')

# 加载标签映射文件，--------------这里直接转为int型
with open("labels.json", "r", encoding="utf-8") as f:
    label_data = json.load(f)
label_mapping = {int(item["label"]): item["label_des"] for item in label_data}

# 加载测试数据
with open("test_processed.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)
print('finish loading json!')
# 预测并转换标签为label_desc
for item in test_data:
    sentence = item["sentence"]
    keywords = item["keywords"]

    # 使用tokenizer对句子进行编码
    encoded_data = tokenizer.encode_plus(
        sentence,
        padding="max_length",
        max_length=64,
        truncation=True,
        return_tensors="pt"
    )

    input_ids = encoded_data["input_ids"].squeeze(0)
    attention_mask = encoded_data["attention_mask"].squeeze(0)
    step=0
    if step % 1000 == 1:
        print(f"Step: {step}/{len(test_data)}")
    step += 1

    # 预测标签
    with torch.no_grad():
        output = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))
        _, predicted = torch.max(output, 1)
        predicted_label = predicted.item()

    # 转换为label_desc
    label_desc = label_mapping[predicted_label]

    # 将label_desc输出到keywords字段
    item["keywords"] = label_desc

# 保存结果到test.json文件
with open("test_processed.json", "w", encoding="utf-8") as f:
    json.dump(test_data, f, ensure_ascii=False, indent=4)

print("预测结果已保存到test.json文件。")

```



### 得分

总的得分为（69%+57.8%+55.8%）/3=61.6%



## 3.做自己的模型

### 3.1 如法炮制的训练代码（以iflynet数据集为例）

```py
import json
import torch
from torch import nn
from transformers import BertTokenizer#(自己建的tokenizer总是莫名报错，写不好，这里无奈借用了BertTokenizer)
from model import Transformer

# 加载字符集
# 加载数据集———————————————————————————————————————————————————————
dataset = [
    "[BEGIN]事实证明 8M 参[MASK]就能做[MASK]差强人意的模型出来。[END]",
    "[BEGIN]这是另一个例子，你可以根据需要添加更多的句子。[END]"
]
with open('234.txt', 'r', encoding='utf-8') as f:  # 与训练时一致
    lines = f.readlines()
    for line in lines:
        dataset.append(line.strip())
print('finish loading dataset!')

# 构建词汇表---------------------------------------------------
vocab = set()
for sentence in dataset:
    vocab.update(list(sentence))
vocab = list(vocab)
vocab_size = 6062

with open("vocab.json", "w", encoding="utf-8") as f:
    json.dump(vocab, f)
with open("vocab.json", "r", encoding="utf-8") as f:
    vocab = json.load(f)

print("词汇表加载完成！")  # 输出vocab_size的大小
char2idx = {c: i for i, c in enumerate(vocab)}
idx2char = {i: c for i, c in enumerate(vocab)}
# 添加未知字符
unknown_token = "[UNK]"
vocab.append(unknown_token)
char2idx[unknown_token] = len(vocab) - 1
idx2char[len(vocab) - 1] = unknown_token
unknown_token_idx = char2idx[unknown_token]


# 加载数据集
def load_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data


# 问题检查(句子中可能出现未标记的字符)
def check_data_labels(data):
    missing_labels = []
    for i, item in enumerate(data):
        if "label" not in item:
            missing_labels.append(i)
    return missing_labels


# 数据预处理
def preprocess_data(data, tokenizer):
    processed_data = []
    
    for item in data:
        label = int(item["label"])
        sentence = item["sentence"]

        # 使用tokenizer对句子进行编码
        encoded_data = tokenizer.encode_plus(
            sentence,
            padding="max_length",
            max_length=64,
            truncation=True,
            return_tensors="pt"
        )
 '''#如果是afqmc数据集：
 	 for item in data:
        sentence1 = item["sentence1"]
        sentence2 = item["sentence2"]
        label = int(item["label"])

        # 使用tokenizer对句子进行编码
        encoded_data = tokenizer.encode_plus(
            sentence1,
            sentence2,
            padding="max_length",
            max_length=64,
            truncation=True,
            return_tensors="pt"
        )'''

        input_ids = encoded_data["input_ids"].squeeze(0)
        attention_mask = encoded_data["attention_mask"].squeeze(0)

        #----------------------- 确保input_ids和attention_mask的值不超出词汇表的索引范围
        input_ids = torch.clamp(input_ids, max=vocab_size - 1)
        attention_mask = torch.clamp(attention_mask, max=1)

        processed_data.append({
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "label": label
        })
    return processed_data


# 创建分类模型
class Classifier(nn.Module):
    def __init__(self, model, output_dim):
        super(Classifier, self).__init__()
        self.model = model
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(model.embed.embedding_dim, output_dim)

    def forward(self, input_ids):
        x = self.model.embed(input_ids)  # 将文本转换为整数索引序列
        x = self.model.positional_encoding(x)
        for transformer_block in self.model.transformer_blocks:
            x = transformer_block(x)
        x = x.mean(dim=1)  # 取平均作为句子表示
        output = self.dropout(x)
        output = self.fc(output)
        return output


# 训练模型
def train_model(model, train_data, dev_data):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

    best_acc = 0.0

    for epoch in range(10):
        print(f"Epoch: {epoch + 1}/10")
        model.train()
        train_loss = 0.0
        train_correct = 0
        step = 0
        for item in train_data:
            input_ids = item["input_ids"].unsqueeze(0).to(device)
            label = torch.tensor(item["label"]).unsqueeze(0).to(device)

            optimizer.zero_grad()

            output = model(input_ids)
            _, predicted = torch.max(output, 1)

            loss = criterion(output, label)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * input_ids.size(0)
            train_correct += (predicted == label).sum().item()
            if step % 1000 == 1:
                print(f"Step: {step}/{len(train_data)}")
            step += 1
        train_loss /= len(train_data)
        train_acc = train_correct / len(train_data)

        val_loss, val_acc = evaluate_model(model, dev_data)

        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print()

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), "best_model_self.pth")

    print("Training finished.")
    print("Best validation accuracy:", best_acc)


# 在验证集上评估模型
def evaluate_model(model, data):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()

    total_loss = 0.0
    total_correct = 0

    with torch.no_grad():
        for item in data:
            input_ids = item["input_ids"].unsqueeze(0).to(device)
            label = torch.tensor(item["label"]).unsqueeze(0).to(device)

            output = model(input_ids)
            _, predicted = torch.max(output, 1)

            loss = nn.CrossEntropyLoss()(output, label)

            total_loss += loss.item() * input_ids.size(0)
            total_correct += (predicted == label).sum().item()

    avg_loss = total_loss / len(data)
    accuracy = total_correct / len(data)

    return avg_loss, accuracy


# 加载测试集并进行评估
def evaluate_test_data(model, test_data):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()

    total_correct = 0

    results = []

    with torch.no_grad():
        for item in test_data:
            input_ids = item["input_ids"].unsqueeze(0).to(device)
            label = torch.tensor(item["label"]).unsqueeze(0).to(device)

            output = model(input_ids)
            _, predicted = torch.max(output, 1)

            results.append({
                "sentence": item["sentence"],
                "predicted_label": predicted.item()
            })

            total_correct += (predicted == label).sum().item()

    accuracy = total_correct / len(test_data)

    return accuracy, results


# 设置文件路径
train_file = "train_processed.json"  # 训练集文件路径
dev_file = "dev_processed.json"  # 验证集文件路径
test_file = "test_processed.json"  # 测试集文件路径

# 加载并预处理数据
train_data = load_data(train_file)
dev_data = load_data(dev_file)
# test_data = load_data(test_file)

tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
train_data = preprocess_data(train_data, tokenizer)
dev_data = preprocess_data(dev_data, tokenizer)
# test_data = preprocess_data(test_data, tokenizer)

# 创建模型实例
vocab_size = 6062
embed_dim = 512
num_heads = 4
hidden_dim = 256
num_layers = 4
model = Transformer(vocab_size, embed_dim, num_heads, hidden_dim, num_layers)
classifier = Classifier(model, output_dim=119)'''剩下两个分别是2和17'''

# 训练模型
train_model(classifier, train_data, dev_data)

# 加载在验证集上表现最好的模型
classifier.load_state_dict(torch.load("best_model_self.pth"))

# 评估测试集
# test_accuracy, test_results = evaluate_test_data(classifier, test_data)

# 打印测试集正确率
# print("Test Accuracy:", test_accuracy)

# 打印测试集结果
print("Test Results:")
# print(test_results[0])

```

分别跑了10轮之后在验证集上评估的结果为48.2%(afqmc),46.7%(iflynet),42.5%(tnews).(惨呀)

注：这里我并没有调用之前已有的model.pth文件,后来我改了模型里的多头数（num_heads=8）

结果略有好转（52.2%(afqmc),47.5%(iflynet),46.5%(tnews)）

### 得分

根据验证集的结果可以暂时评价一下得分，大致为45.8%

### 3.2模型测试输出（以iflynet数据集为例）



将以上代码存为selftrain.py，那么在模型测试输出的时候只要修改以下部分：

```py
import json
import torch
from torch import LongTensor
from transformers import BertTokenizer, BertModel
from train import Classifier

# 加载BERT模型和tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert = BertModel.from_pretrained("bert-base-chinese")
output_dim = 17  # 输出维度（分类数目）
print('finish loading bert!')
```



改为

```py
import json
import torch
from transformers import BertTokenizer
from selftrain import Classifier
from model import Transformer

# 加载自己的模型和tokenizer
# 加载字符集
# 加载数据集———————————————————————————————————————————————————————
dataset = [
    "[BEGIN]事实证明 8M 参[MASK]就能做[MASK]差强人意的模型出来。[END]",
    "[BEGIN]这是另一个例子，你可以根据需要添加更多的句子。[END]"
]
with open('234.txt', 'r', encoding='utf-8') as f:  # 与训练时一致
    lines = f.readlines()
    for line in lines:
        dataset.append(line.strip())
print('finish loading dataset!')

# 构建词汇表---------------------------------------------------
vocab = set()
for sentence in dataset:
    vocab.update(list(sentence))
vocab = list(vocab)
vocab_size = 6062

with open("vocab.json", "w", encoding="utf-8") as f:
    json.dump(vocab, f)
with open("vocab.json", "r", encoding="utf-8") as f:
    vocab = json.load(f)

print("词汇表加载完成！")  # 输出vocab_size的大小
char2idx = {c: i for i, c in enumerate(vocab)}
idx2char = {i: c for i, c in enumerate(vocab)}
# 添加未知字符
unknown_token = "[UNK]"
vocab.append(unknown_token)
char2idx[unknown_token] = len(vocab) - 1
idx2char[len(vocab) - 1] = unknown_token
unknown_token_idx = char2idx[unknown_token]
```



## 4.总结

在本轮实践中我学到了数据预处理的新方式(用tokenizer进行编码)：

```py
# 数据预处理
def preprocess_data(data, tokenizer):
    processed_data = []
    
    for item in data:
        label = int(item["label"])
        sentence = item["sentence"]

        # 使用tokenizer对句子进行编码
        encoded_data = tokenizer.encode_plus(
            sentence,
            padding="max_length",
            max_length=64,
            truncation=True,
            return_tensors="pt"
        )

        input_ids = encoded_data["input_ids"].squeeze(0)#降维
        attention_mask = encoded_data["attention_mask"].squeeze(0)

        # 确保input_ids和attention_mask的值不超出词汇表的索引范围
        input_ids = torch.clamp(input_ids, max=vocab_size - 1)
        attention_mask = torch.clamp(attention_mask, max=1)
		#将处理后的input_ids、attention_mask和标签label存储为字典形式，并将其添加到processed_data列表中
        processed_data.append({
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "label": label
        })
    return processed_data
```

以及分类模型的创建：

```py
# 创建分类模型
class Classifier(nn.Module):
    def __init__(self, model, output_dim):
        super(Classifier, self).__init__()
        self.model = model
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(model.embed.embedding_dim, output_dim)

    def forward(self, input_ids):
        x = self.model.embed(input_ids)  # 将文本转换为整数索引序列
        x = self.model.positional_encoding(x)
        for transformer_block in self.model.transformer_blocks:
            x = transformer_block(x)
        x = x.mean(dim=1)                 # 取平均作为句子表示
        output = self.dropout(x)
        output = self.fc(output)          #前向传播
        return output
```



为什么要引入Transformer中的tokenizer（BertTokenizer）：

tokenizer用于将输入的文本句子转换为模型所需的输入格式，例如将句子编码为整数索引序列和生成注意力掩码。

Transformer模型需要输入被编码为整数索引的句子表示，因此需要使用tokenizer将句子转换为这种表示形式。

引入BertTokenizer是因为它是用于处理中文文本的预训练模型(BERT)的tokenizer，它提供了适用于中文文本的编码方法。在这个例子中，BertTokenizer被用于编码和预处理中文句子。（BertTokenizer真的好用）



































